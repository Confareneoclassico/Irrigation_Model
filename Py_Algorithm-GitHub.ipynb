{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import sobol_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a few functions to be used in the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Y(Y0,Beta,Epsilon,r,N0,Gamma,Year0):\n",
    "    return Y0+10**(a+b*Beta+Epsilon)*Beta*r*((N0*1000)**(1-Gamma)+r*(2050-Year0)*(1-Gamma))**((Beta+Gamma-1)/\n",
    "    (1-Gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(key, values):\n",
    "    return dict(zip(key, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_index(t):\n",
    "    return np.sum(t*[len(lu)/len(Areas2),len(lu)/(len(Areas2)*len(dataSets)),len(lu)/(len(Areas2)*len(dataSets)*\n",
    "len(Regression_Method)),len(lu)/(len(Areas2)*len(dataSets)*len(Regression_Method)*len(Robustness)),1],axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = pd.read_csv('sample.matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = p3.replace({'Continent':{1:'Africa',2:'Americas',3:'Asia',4:'Europe'},\n",
    "'X1':{1:'Aquastat',2:'FAOSTAT',3:'Siebert.et.al.2013',4:'Meier.et.al.2018',5:'Salmon.et.al.2015',6:'Thenkabail.et.al.2009'},\n",
    "'X2':{1:'OLS',2:'SMA'},'X3':{1:'NO',2:'YES'}})\n",
    "p3 = p3.rename(index=str, columns={\"X1\":\"Dataset\",\"X2\":\"Regression\",\"X3\":\"Robust\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lu = pd.read_csv('lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lu = lu.replace({'Continent':{1:'Africa',2:'Americas',3:'Asia',4:'Europe'},\n",
    "'Dataset':{1:'Aquastat',2:'FAOSTAT',3:'Siebert.et.al.2013',4:'Meier.et.al.2018',5:'Salmon.et.al.2015',6:'Thenkabail.et.al.2009'},\n",
    "'Regression':{1:'OLS',2:'SMA'},'Robust':{1:'NO',2:'YES'}})\n",
    "Continent = lu['Continent']\n",
    "lu.drop(labels=['Continent'], axis=1,inplace = True)\n",
    "lu.insert(0, 'Continent', Continent)\n",
    "lu = lu.sort_values(by=['Continent','Dataset','Regression','Robust','Beta'])\n",
    "lu = lu.set_index(['Continent','Dataset','Regression','Robust']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lu = lu.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the irrigated area - Y0 in our simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrArea = pd.read_excel('full.dataset2.xlsx',\\\n",
    "                      sheet_name='meier')\n",
    "\n",
    "Areas2 = ['Africa','Americas','Asia','Europe']\n",
    "dataSets = ['Aquastat','FAOSTAT','Meier.et.al.2018','Salmon.et.al.2015','Siebert.et.al.2013','Thenkabail.et.al.2009']\n",
    "Regression_Method = ['OLS','SMA']\n",
    "Robustness = ['YES','NO']\n",
    "sl = [['OLS','NO'],['OLS','YES'],['SMA','NO'],['SMA','YES']]\n",
    "Y0 = pd.DataFrame([irrArea[irrArea['Continent']==ar2].sum() for ar2 in Areas2], index=[ar2 for ar2 in Areas2])\n",
    "Y0=Y0.iloc[:,3:]\n",
    "Y0=Y0.reindex(sorted(Y0.columns), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The K setting the carrying capacity factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = pd.DataFrame([[7.14e8,8.85e8,6.68e8,4.78e8],[1.205e9,1.481e9,8.89e8,7.05e8]],columns=[ar2 for ar2 in Areas2],index=\\\n",
    "                    ['Net','Gross']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = pd.read_csv('out.df.csv').set_index(['Continent','Dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quasi-random matrix for the Monte Carlo simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qM =sobol_seq.i4_sobol_generate(21,100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the triggers for the lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triggers = np.vstack((np.array(list(itertools.chain.from_iterable(itertools.repeat(x, 100000) \n",
    "for x in range(len(Areas2))))),np.tile((qM[:100000,0:4]*[6,2,1,20000]).T,4)))\n",
    "\n",
    "triggersB = np.vstack((np.array(list(itertools.chain.from_iterable(itertools.repeat(x, 100000) \n",
    "for x in range(len(Areas2))))),np.tile((qM[:100000,10:14]*[6,2,1,20000]).T,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = np.array([0 if t2[3] > 1/(2+outliers['Outliers'][ar2,dataSets[t2[1].astype(int)]]) else 1 for ar2 in Areas2 \n",
    "               for t2 in triggers[:,:100000].T])\n",
    "lb = np.array([0 if t2b[3] > 1/(2+outliers['Outliers'][ar2,dataSets[t2b[1].astype(int)]]) else 1 for ar2 in Areas2 \n",
    "               for t2b in triggersB[:,:100000].T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triggers[3]=la\n",
    "triggersB[3]=lb\n",
    "triggers = triggers.astype(int)\n",
    "triggersB = triggersB.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust the database to 100k values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = pd.concat([pd.Series(p3.r[p3.Continent==ar2].iloc[:int(len(p3[p3.Continent==ar2])/2)]) for ar2 in Areas2])\n",
    "p4B = pd.concat([pd.Series(p3.r[p3.Continent==ar2].iloc[int(len(p3[p3.Continent==ar2])/2):]) for ar2 in Areas2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the coefficients for the alpha regression in terms of Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5.51242451040592\n",
    "b = -7.24661215514772\n",
    "epsilon = 0.157964920922199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population initial figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPop = pd.read_csv('UN_PP1999-2012.csv')\n",
    "del PPop['Variant']\n",
    "PPop = PPop.rename(columns={'Country or Area':'Continent','Year(s)':'Year'})\n",
    "PPop=PPop.pivot(index='Continent',columns='Year',values='Value')\n",
    "PPop.loc['Americas']=PPop.loc['Latin America and the Caribbean']+PPop.loc['Northern America']\n",
    "PPop = PPop.drop(['Latin America and the Caribbean','Northern America'])\n",
    "PPop = PPop.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_As = [triggers] \n",
    "for tr in range(1,len(triggers)):\n",
    "    tr_As.append(triggers.copy())\n",
    "    tr_As[tr][tr]=triggersB[tr]\n",
    "    \n",
    "tr_Bs = [triggersB] \n",
    "for trB in range(1,len(triggers)):\n",
    "    tr_Bs.append(triggersB.copy())\n",
    "    tr_Bs[trB][trB]=triggers[trB]\n",
    "    \n",
    "tIndexA = [trigger_index(tri.T) for tri in tr_As]\n",
    "tIndexB = [trigger_index(triB.T) for triB in tr_Bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sM_As = []\n",
    "for itI,tI in enumerate(tIndexA):\n",
    "    sM_As.append(lu.copy())\n",
    "    sM_As[itI]=sM_As[itI].reindex(tI)\n",
    "    sM_As[itI]=sM_As[itI].reset_index(drop=True)\n",
    "    \n",
    "sM_Bs = []\n",
    "for itIB,tIB in enumerate(tIndexB):\n",
    "    sM_Bs.append(lu.copy())\n",
    "    sM_Bs[itIB]=sM_Bs[itIB].reindex(tIB)\n",
    "    sM_Bs[itIB]=sM_Bs[itIB].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sm in sM_As:\n",
    "    sm['r']=p4.values\n",
    "    sm['Epsilon']=np.tile(epsilon*2**0.5*sp.special.erfinv(2*qM[:,6]-1),4)\n",
    "    sm['Year0'] = np.tile((PPop.columns.min()+(PPop.columns.max()-PPop.columns.min()+1)*qM[:,7]).astype(int),4)\n",
    "    sm['Gamma']= np.tile(0.02*2**0.5*sp.special.erfinv(2*qM[:,8]-1)+1,4)\n",
    "    sm['Y0']=0\n",
    "    sm['K']=0\n",
    "    for ar2 in Areas2:\n",
    "        sm['Y0'][sm['Continent']==ar2]=Y0.loc[ar2].min()+qM[:,5]*(Y0.loc[ar2].max()-Y0.loc[ar2].min())\n",
    "        sm['K'][sm['Continent']==ar2]=K['Net'].loc[ar2]+qM[:,9]*(K['Gross'].loc[ar2]-K['Net'].loc[ar2])\n",
    "    sm['N0']=[PPop[smy][ar2] for ar2 in Areas2 for smy in sm['Year0'][:int(len(sm)/4)]]\n",
    "        \n",
    "for smB in sM_Bs:\n",
    "    smB['r']=p4B.values\n",
    "    smB['Epsilon']=np.tile(epsilon*2**0.5*sp.special.erfinv(2*qM[:,16]-1),4)\n",
    "    smB['Year0'] = np.tile((PPop.columns.min()+(PPop.columns.max()-PPop.columns.min()+1)*qM[:,17]).astype(int),4)\n",
    "    smB['Gamma']= np.tile(0.02*2**0.5*sp.special.erfinv(2*qM[:,18]-1)+1,4)\n",
    "    smB['Y0']=0\n",
    "    smB['K']=0\n",
    "    for ar2 in Areas2:\n",
    "        smB['Y0'][smB['Continent']==ar2]=Y0.loc[ar2].min()+qM[:,15]*(Y0.loc[ar2].max()-Y0.loc[ar2].min())\n",
    "        smB['K'][smB['Continent']==ar2]=K['Net'].loc[ar2]+qM[:,19]*(K['Gross'].loc[ar2]-K['Net'].loc[ar2])\n",
    "    smB['N0']=[PPop[smy][ar2] for ar2 in Areas2 for smy in smB['Year0'][:int(len(smB)/4)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the variables list for the scrambled matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables2 = [['r'],['Y0'],['Epsilon'],['Year0','N0'],['Gamma'],['K']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [['Dataset'],['Regression'],['Robust'],['X4'],['r'],['Y0'],['Epsilon'],['Year0','N0'],['Gamma'],['K']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iv,v in enumerate(variables2):\n",
    "    sM_As.append(sM_As[0].copy())\n",
    "    sM_As[-1][v]=sM_Bs[0][v]\n",
    "    \n",
    "    sM_Bs.append(sM_Bs[0].copy())\n",
    "    sM_Bs[-1][v]=sM_As[0][v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Y and replace overshooting Y values with K figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sm in sM_As:\n",
    "    sm['Y']=Y(sm.Y0,sm.Beta,sm.Epsilon,sm.r,sm.N0,sm.Gamma,sm.Year0)\n",
    "    sm['Y'].loc[sm['Y']>sm.K] = sm.K\n",
    "\n",
    "for smB in sM_Bs:\n",
    "    smB['Y']=Y(smB.Y0,smB.Beta,smB.Epsilon,smB.r,smB.N0,smB.Gamma,smB.Year0)\n",
    "    smB['Y'].loc[smB['Y']>smB.K] = smB.K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sM_As = [sm[sm['Y']>0] for sm in sM_As]\n",
    "\n",
    "sM_Bs = [smB[smB['Y']>0] for smB in sM_Bs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append the B matrix for the Sobol-indices computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sM_S = sM_As.copy()\n",
    "sM_S.append(sM_Bs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow down the indices in order to get rid of potential 'orphan' (uncoupled) rows across the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inr = sM_S[0].index.intersection(sM_S[1].index).intersection(sM_S[2].index).intersection(sM_S[3].index).\\\n",
    "intersection(sM_S[4].index).intersection(sM_S[5].index).intersection(sM_S[6].index).intersection(sM_S[7].index).\\\n",
    "intersection(sM_S[8].index).intersection(sM_S[9].index).intersection(sM_S[10].index).intersection(sM_S[11].index)\n",
    "\n",
    "sM_S = [sms.merge(pd.DataFrame(index=inr), left_index=True, right_index=True, how='right') for sms in sM_S]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess the Global Irrigated Area in 2050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldIrrigatedArea = pd.concat([sM_As[0].Y[ia*int(len(sM_As[0].Y)/len(Areas2)):(ia+1)*int(len(sM_As[0].Y)/len(Areas2))].reset_index(drop=True) \n",
    "                                for ia in range(len(Areas2))],axis=1,ignore_index=True).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the statistical properties of the datasets used for the four continents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for bootstrapping\n",
    "qMbS = sobol_seq.i4_sobol_generate(1,len(sM_As[0])*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap sensitivity indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "c_r = []\n",
    "T_r = []\n",
    "Sa_r = []\n",
    "for r in range(1000):    \n",
    "    seed = (qMbS[r*len(sM_As[0]):(r+1)*len(sM_As[0]),0]*len(sM_As[0])).astype(int)\n",
    "    \n",
    "    sM_BS = [smS.merge(pd.DataFrame(index=seed), left_index=True, right_index=True, how='right') for smS in sM_S]\n",
    "\n",
    "    V = []\n",
    "    Sa_V = []\n",
    "    T_V = []\n",
    "    for imb,mb in enumerate(sM_BS[1:-1]):\n",
    "        Geo = []\n",
    "        Sa_G = []\n",
    "        T_G = []\n",
    "        for ar2 in Areas2:\n",
    "            Sa_G.append(((mb['Y'][mb['Continent']==ar2]-sM_BS[0]['Y'][sM_BS[0]['Continent']==ar2])* sM_BS[-1]['Y'][sM_BS[-1]['Continent']==ar2]).mean()/\\\n",
    "            pd.concat([sM_BS[-1]['Y'][sM_BS[-1]['Continent']==ar2],sM_BS[0]['Y'][sM_BS[0]['Continent']==ar2]]).var(ddof=0))\n",
    "            T_G.append(0.5*((mb['Y'][mb['Continent']==ar2]-sM_BS[0]['Y'][sM_BS[0]['Continent']==ar2])**2).mean()/\\\n",
    "            pd.concat([sM_BS[-1]['Y'][sM_BS[-1]['Continent']==ar2],sM_BS[0]['Y'][sM_BS[0]['Continent']==ar2]]).var(ddof=0))\n",
    "            Geo.append(ar2)\n",
    "        Sa_G_dic = create_dict(Geo,Sa_G)\n",
    "        T_G_dic = create_dict(Geo,T_G)\n",
    "        V.append(variables[imb][0])\n",
    "        Sa_V.append(Sa_G_dic)\n",
    "        T_V.append(T_G_dic)\n",
    "    Sa_V_dic = create_dict(V,Sa_V)\n",
    "    T_V_dic = create_dict(V,T_V)\n",
    "    \n",
    "    c_r.append(r)\n",
    "    Sa_r.append(Sa_V_dic)\n",
    "    T_r.append(T_V_dic)\n",
    "Sa_r_dic = create_dict(c_r,Sa_r)\n",
    "T_r_dic = create_dict(c_r,T_r)\n",
    "Sa_of_df = {Sa_k: pd.DataFrame(Sa_v) for Sa_k,Sa_v in Sa_r_dic.items()}\n",
    "T_of_df = {T_k: pd.DataFrame(T_v) for T_k,T_v in T_r_dic.items()}\n",
    "T_df = pd.concat(T_of_df, axis=0)\n",
    "Sa_df = pd.concat(Sa_of_df, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irrigated Land Area Distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = ['b','r','g','k','c','m']\n",
    "plt.style.use('ggplot')\n",
    "for ar2 in Areas2:\n",
    "    ax = sns.distplot(sM_As[0].Y[sM_As[0].Continent==ar2], bins=500, kde=False, axlabel='Irrigated Area 2050 (ha)_'+str(ar2))\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel('Count')\n",
    "    for ds in K:\n",
    "        plt.axvline(x=K[ds][ar2],label=str(ds)+' Cropland in 2050')\n",
    "    plt.legend()\n",
    "    plt.savefig('Uncertainty Analysis_'+str(ar2))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution plots World Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "ax = sns.distplot(worldIrrigatedArea, bins=200, kde=False, axlabel='Global Irrigated Area 2050 (ha)')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('Count')\n",
    "for ds in Y0:\n",
    "    plt.plot([Y0[ds].sum(),Y0[ds].sum()], [0,16000], '--', linewidth=2, label=ds)\n",
    "plt.yticks(np.arange(0, 2e4, step=5e3))\n",
    "plt.legend()\n",
    "plt.savefig('Uncertainty Analysis_World')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = ['Dataset','Regression','Robust','r','Y0','Epsilon','Year0','Gamma','K']\n",
    "for ar2 in Areas2:\n",
    "    for col in co:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.scatter(sM_As[0][col][sM_As[0].Continent==ar2],sM_As[0]['Y'][sM_As[0].Continent==ar2],s=1,label=str(ar2)+'_'+str(col))\n",
    "        plt.yscale('log')\n",
    "        plt.ylim(1e6,2e9)\n",
    "        plt.legend()\n",
    "        plt.savefig('ScatterPlot_'+str(col)+str(ar2))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity-indices boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ar2 in Areas2:\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    for iv,v in enumerate(variables):\n",
    "        T_df.loc[pd.IndexSlice[:, ar2],:].plot(kind='box',ax=ax,label='T', color='b',patch_artist=True)\n",
    "        Sa_df.loc[pd.IndexSlice[:, ar2],:].plot(kind='box',ax=ax,label='S',positions=[iv+0.5 for iv in range(len(variables))],\n",
    "                                              color='r',patch_artist=True)\n",
    "    m_patch = mpatches.Patch(color='b', label='T')\n",
    "    c_patch = mpatches.Patch(color='r', label='S')\n",
    "    plt.legend(handles=[m_patch,c_patch])\n",
    "    plt.xticks([iv+0.75 for iv in range(len(variables))])\n",
    "    plt.xlim(0,10.5)\n",
    "    plt.title(ar2)\n",
    "    plt.savefig('Confidence_Interval_Sobol_Indices_'+str(ar2))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining variables for the sensitivity analysis of the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables3 = [['Dataset','Y0'],['Regression','Robust','X4','Epsilon'],['r','Gamma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_As2 = [triggers] \n",
    "for tr2 in range(1,len(variables3)):\n",
    "    tr_As2.append(triggers.copy())\n",
    "    \n",
    "tr_Bs2 = [triggersB] \n",
    "for trB2 in range(1,len(variables3)):\n",
    "    tr_Bs2.append(triggersB.copy())\n",
    "\n",
    "tr_As2[1][1:4]=tr_Bs2[0][1:4]\n",
    "tr_As2[-1][-1]=tr_Bs2[0][-1]\n",
    "\n",
    "tr_Bs2[1][1:4]=tr_As2[0][1:4]\n",
    "tr_Bs2[-1][-1]=tr_As2[0][-1]\n",
    "\n",
    "tIndexA2 = [trigger_index(tri2.T) for tri2 in tr_As2]\n",
    "tIndexB2 = [trigger_index(triB2.T) for triB2 in tr_Bs2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sM_As2 = []\n",
    "for itI2,tI2 in enumerate(tIndexA2):\n",
    "    sM_As2.append(lu.copy())\n",
    "    sM_As2[itI2]=sM_As2[itI2].reindex(tI2)\n",
    "    sM_As2[itI2]=sM_As2[itI2].reset_index(drop=True)\n",
    "    \n",
    "sM_Bs2 = []\n",
    "for itIB2,tIB2 in enumerate(tIndexB2):\n",
    "    sM_Bs2.append(lu.copy())\n",
    "    sM_Bs2[itIB2]=sM_Bs2[itIB2].reindex(tIB2)\n",
    "    sM_Bs2[itIB2]=sM_Bs2[itIB2].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sm2 in sM_As2:\n",
    "    sm2['r']=p4.values\n",
    "    sm2['Epsilon']=np.tile(epsilon*2**0.5*sp.special.erfinv(2*qM[:,6]-1),4)\n",
    "    sm2['Year0'] = np.mean(PPop.columns).astype(int)\n",
    "    sm2['Gamma']= np.tile(0.02*2**0.5*sp.special.erfinv(2*qM[:,8]-1)+1,4)\n",
    "    sm2['Y0']=0\n",
    "    sm2['K']=0\n",
    "    sm2['N0']=0\n",
    "    for ar2 in Areas2:\n",
    "        sm2['Y0'][sm2['Continent']==ar2]=Y0.loc[ar2].min()+qM[:,5]*(Y0.loc[ar2].max()-Y0.loc[ar2].min())\n",
    "        sm2['K'][sm2['Continent']==ar2]=np.mean([K['Net'].loc[ar2],K['Gross'].loc[ar2]])\n",
    "        sm2['N0'][sm2['Continent']==ar2]=PPop[np.mean(PPop.columns).astype(int)].loc[ar2]\n",
    "        \n",
    "for smB2 in sM_Bs2:\n",
    "    smB2['r']=p4B.values\n",
    "    smB2['Epsilon']=np.tile(epsilon*2**0.5*sp.special.erfinv(2*qM[:,16]-1),4)\n",
    "    smB2['Year0'] = np.mean(PPop.columns).astype(int)\n",
    "    smB2['Gamma']= np.tile(0.02*2**0.5*sp.special.erfinv(2*qM[:,18]-1)+1,4)\n",
    "    smB2['Y0']=0\n",
    "    smB2['K']=0\n",
    "    smB2['N0']=0\n",
    "    for ar2 in Areas2:\n",
    "        smB2['Y0'][smB2['Continent']==ar2]=Y0.loc[ar2].min()+qM[:,15]*(Y0.loc[ar2].max()-Y0.loc[ar2].min())\n",
    "        smB2['K'][smB2['Continent']==ar2]=np.mean([K['Net'].loc[ar2],K['Gross'].loc[ar2]])\n",
    "        smB2['N0'][smB2['Continent']==ar2]=PPop[np.mean(PPop.columns).astype(int)].loc[ar2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the scrambled matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sM_As2.append(sM_As2[0].copy())\n",
    "sM_Bs2.append(sM_Bs2[0].copy())\n",
    "\n",
    "sM_As2[1]['Y0']=sM_Bs2[0]['Y0']\n",
    "sM_As2[2]['Epsilon']=sM_Bs2[0]['Epsilon']\n",
    "sM_As2[-1][['Gamma','r']]=sM_Bs2[0][['Gamma','r']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Y and replace overshooting Y values with K figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sm2 in sM_As2:\n",
    "    sm2['Y']=Y(sm2.Y0,sm2.Beta,sm2.Epsilon,sm2.r,sm2.N0,sm2.Gamma,sm2.Year0)\n",
    "    sm2['Y'].loc[sm2['Y']>sm2.K] = sm2.K\n",
    "    \n",
    "for smB2 in sM_Bs2:\n",
    "    smB2['Y']=Y(smB2.Y0,smB2.Beta,smB2.Epsilon,smB2.r,smB2.N0,smB2.Gamma,smB2.Year0)\n",
    "    smB2['Y'].loc[smB2['Y']>smB2.K] = smB2.K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sM_As2 = [sm2[sm2['Y']>0] for sm2 in sM_As2]\n",
    "\n",
    "sM_Bs2 = [smB2[smB2['Y']>0] for smB2 in sM_Bs2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sM_S2 = sM_As2.copy()\n",
    "sM_S2.append(sM_Bs2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inr2 = sM_S2[0].index.intersection(sM_S2[1].index).intersection(sM_S2[2].index).intersection(sM_S2[3].index).\\\n",
    "intersection(sM_S2[4].index)\n",
    "\n",
    "sM_S2 = [sms2.merge(pd.DataFrame(index=inr2), left_index=True, right_index=True, how='right') for sms2 in sM_S2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append B matrix for the Sobol indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for bootstrapping\n",
    "qMbS = sobol_seq.i4_sobol_generate(1,len(sM_S2[0])*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_r2 = []\n",
    "T_r2 = []\n",
    "Sa_r2 = []\n",
    "for r in range(1000):    \n",
    "    seed = (qMbS[r*len(sM_As2[0]):(r+1)*len(sM_As2[0]),0]*len(sM_As2[0])).astype(int)\n",
    "    \n",
    "    sM_BS2 = [sms2.merge(pd.DataFrame(index=seed), left_index=True, right_index=True, how='right') for sms2 in sM_S2]\n",
    "\n",
    "    V = []\n",
    "    Sa_V = []\n",
    "    T_V = []\n",
    "    for imb,mb in enumerate(sM_BS2[1:-1]):\n",
    "        Geo = []\n",
    "        Sa_G = []\n",
    "        T_G = []\n",
    "        for ar2 in Areas2:\n",
    "            Sa_G.append(((mb['Y'][mb['Continent']==ar2]-sM_BS2[0]['Y'][sM_BS2[0]['Continent']==ar2])*\n",
    "            sM_BS2[-1]['Y'][sM_BS2[-1]['Continent']==ar2]).mean()/\\\n",
    "            pd.concat([sM_BS2[-1]['Y'][sM_BS2[-1]['Continent']==ar2],sM_BS2[0]['Y'][sM_BS2[0]['Continent']==ar2]]).var(ddof=0))\n",
    "            T_G.append(0.5*((mb['Y'][mb['Continent']==ar2]-sM_BS2[0]['Y'][sM_BS2[0]['Continent']==ar2])**2).mean()/\\\n",
    "            pd.concat([sM_BS2[-1]['Y'][sM_BS2[-1]['Continent']==ar2],sM_BS2[0]['Y'][sM_BS2[0]['Continent']==ar2]]).var(ddof=0))\n",
    "            Geo.append(ar2)\n",
    "        Sa_G_dic = create_dict(Geo,Sa_G)\n",
    "        T_G_dic = create_dict(Geo,T_G)\n",
    "        V.append(variables3[imb][0])\n",
    "        Sa_V.append(Sa_G_dic)\n",
    "        T_V.append(T_G_dic)\n",
    "    Sa_V_dic = create_dict(V,Sa_V)\n",
    "    T_V_dic = create_dict(V,T_V)\n",
    "    \n",
    "    c_r2.append(r)\n",
    "    Sa_r2.append(Sa_V_dic)\n",
    "    T_r2.append(T_V_dic)\n",
    "    \n",
    "Sa_r_dic2 = create_dict(c_r2,Sa_r2)\n",
    "T_r_dic2 = create_dict(c_r2,T_r2)\n",
    "Sa_of_df2 = {Sa_k2: pd.DataFrame(Sa_v2) for Sa_k2,Sa_v2 in Sa_r_dic2.items()}\n",
    "T_of_df2 = {T_k2: pd.DataFrame(T_v2) for T_k2,T_v2 in T_r_dic2.items()}\n",
    "T_df2 = pd.concat(T_of_df2, axis=0)\n",
    "Sa_df2 = pd.concat(Sa_of_df2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sOl = []\n",
    "for cS in Sa_df2:\n",
    "    sOl.append(1-Sa_df2.sum(axis=1)+Sa_df2[cS]-T_df2[cS])\n",
    "secondOrder = pd.concat(sOl, axis=1).rename(columns={0:'S_Model-Population',1:'S_Irrigation-Population',\n",
    "                                                     2:'S_Irrigation-Model'})\n",
    "thirdOrder = T_df2.sum(axis=1)+Sa_df2.sum(axis=1)-2\n",
    "\n",
    "SIs = pd.concat([Sa_df2,secondOrder,thirdOrder],axis=1).rename(columns={'Dataset':'S_Irrigation','Regression':'S_Model',\n",
    "'r':'S_Population',0:'S_third_order'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity indices boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ar2 in Areas2:\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    co = -1\n",
    "    for cSs in SIs:\n",
    "        co+=1\n",
    "        SIs.loc[pd.IndexSlice[:, ar2],:].plot(kind='box',ax=ax,label=cSs,patch_artist=True)\n",
    "    plt.xlim(0,7.5)\n",
    "    plt.ylim(-0.1,0.8)\n",
    "    plt.title(ar2)\n",
    "    plt.savefig('Sensitivity_Indices_Clusters_'+str(ar2))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables4 = ['Irrigation','Model','Population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_df2 = T_df2.rename(columns={'Dataset':'Irrigation','Regression':'Model','r':'Population'})\n",
    "Sa_df2 = Sa_df2.rename(columns={'Dataset':'Irrigation','Regression':'Model','r':'Population'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ar2 in Areas2:\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    for iv,v in enumerate(variables4):\n",
    "        T_df2.loc[pd.IndexSlice[:, ar2],:].plot(kind='box',ax=ax,label='T', color='b',patch_artist=True)\n",
    "        Sa_df2.loc[pd.IndexSlice[:, ar2],:].plot(kind='box',ax=ax,label='S',positions=[iv+0.5 for iv in range(len(variables4))],\n",
    "                                              color='r',patch_artist=True)\n",
    "    m_patch = mpatches.Patch(color='b', label='T')\n",
    "    c_patch = mpatches.Patch(color='r', label='S')\n",
    "    plt.legend(handles=[m_patch,c_patch])\n",
    "    plt.xticks([iv+0.75 for iv in range(len(variables4))])\n",
    "    plt.xlim(0,3.5)\n",
    "    plt.title(ar2)\n",
    "    plt.savefig('Confidence_Interval_Sobol_Indices_Clusters_'+str(ar2))\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
